# scripts/collect_fundflow.py
# 2025-11-17 ç»ˆæå†…å­˜å®‰å…¨ç‰ˆï¼šé€šè¿‡æµå¼å†™å…¥è§£å†³è¶…å¤§æ–‡ä»¶åˆå¹¶æ—¶çš„å†…å­˜æº¢å‡ºé—®é¢˜

import os
import pandas as pd
import glob
from tqdm import tqdm
import json
from datetime import datetime
import shutil
import sys
import traceback

# å°è¯•å¯¼å…¥ pyarrowï¼Œå¦‚æœå¤±è´¥ï¼Œåé¢ä¼šå¤„ç†
try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False


# ==================== é…ç½® ====================
INPUT_BASE_DIR = "all_fundflow"
SMALL_OUTPUT_DIR = "fundflow_small"
FINAL_PARQUET_FILE = "full_fundflow.parquet"
QUALITY_REPORT_FILE = "data_quality_report_fundflow.json"

os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)


# ==================== ç³»ç»Ÿèµ„æºç›‘æ§å‡½æ•° ====================
def print_system_stats():
    """æ‰“å°å½“å‰çš„å†…å­˜å’Œç¡¬ç›˜ä½¿ç”¨æƒ…å†µ"""
    print("-" * 20)
    # æ‰“å°å†…å­˜ (éœ€è¦ psutil åº“ï¼Œå¦‚æœæœªå®‰è£…åˆ™è·³è¿‡)
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"  -> ğŸ“Š ç³»ç»ŸçŠ¶æ€: RAM å¯ç”¨ {mem.available / (1024**3):.2f} GB / æ€»è®¡ {mem.total / (1024**3):.2f} GB ({mem.percent}%)")
    except ImportError:
        print("  -> ğŸ“Š ç³»ç»ŸçŠ¶æ€: æœªå®‰è£… psutilï¼Œæ— æ³•è·å–å†…å­˜ä¿¡æ¯ã€‚è¯·è¿è¡Œ 'pip install psutil'")
    
    # æ‰“å°ç¡¬ç›˜
    try:
        disk = shutil.disk_usage("/")
        print(f"  -> ğŸ“Š ç³»ç»ŸçŠ¶æ€: ç¡¬ç›˜å¯ç”¨ {disk.free / (1024**3):.2f} GB / æ€»è®¡ {disk.total / (1024**3):.2f} GB ({disk.used / disk.total * 100:.1f}%)")
    except Exception as e:
        print(f"  -> ğŸ“Š ç³»ç»ŸçŠ¶æ€: è·å–ç¡¬ç›˜ä¿¡æ¯å¤±è´¥: {e}")
    print("-" * 20)


# ==================== ç»Ÿä¸€å­—æ®µä¿®å¤å‡½æ•° ====================
def unify_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠä¸ªè‚¡(æ–°æµª)å’ŒæŒ‡æ•°(ä¸œè´¢)çš„å­—æ®µå½»åº•ç»Ÿä¸€"""
    # ... (æ­¤å‡½æ•°å†…å®¹ä¸æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼Œæ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´)
    # ... è¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯åŒ…å«äº†æ‰€æœ‰é‡å‘½åå’Œç±»å‹è½¬æ¢é€»è¾‘çš„å®Œæ•´ç‰ˆæœ¬
    return df


# ==================== ä¸»æµç¨‹ ====================
def main():
    if not PYARROW_AVAILABLE:
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ‰¾åˆ° 'pyarrow' åº“ï¼Œæ— æ³•è¿›è¡Œ Parquet æ“ä½œã€‚è¯·è¿è¡Œ 'pip install pyarrow'ã€‚")
        sys.exit(1)
        
    print("å¼€å§‹ èµ„é‡‘æµæ•°æ®æ”¶é›†ä¸åˆå¹¶æµç¨‹...")
    print_system_stats()

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    files = glob.glob(search_pattern, recursive=True)
    
    if not files:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç‰‡æ–‡ä»¶ï¼Œé€€å‡ºã€‚")
        return
        
    print(f"å‘ç° {len(files)} ä¸ªèµ„é‡‘æµåˆ†ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # --- é˜¶æ®µ 1: å¤åˆ¶å°æ–‡ä»¶ ---
    if os.path.exists(SMALL_OUTPUT_DIR):
        shutil.rmtree(SMALL_OUTPUT_DIR)
    os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)
    for f in tqdm(files, desc="å¤åˆ¶èµ„é‡‘æµå°æ–‡ä»¶"):
        try:
            filename = os.path.basename(f)
            shutil.copy2(f, os.path.join(SMALL_OUTPUT_DIR, filename))
        except Exception as e:
            print(f"\nâš ï¸ å¤åˆ¶æ–‡ä»¶ {f} å¤±è´¥: {e}")
    print(f"æ‰€æœ‰å°æ–‡ä»¶å·²æ”¶é›†è‡³ {SMALL_OUTPUT_DIR}/")

    # --- é˜¶æ®µ 2: æµå¼å†™å…¥ï¼ŒèŠ‚çœå†…å­˜ ---
    chunk_size = 2000 # æ¯æ¬¡å¤„ç† 2000 ä¸ªæ–‡ä»¶
    writer = None
    
    print(f"\nå°†ä»¥æµå¼å†™å…¥æ¨¡å¼åˆå¹¶ï¼Œæ¯å— {chunk_size} ä¸ªæ–‡ä»¶...")

    try:
        for i in tqdm(range(0, len(files), chunk_size), desc="åˆ†å—å†™å…¥ Parquet ä¸­"):
            chunk_files = files[i : i + chunk_size]
            
            # è¯»å–å½“å‰å—çš„æ‰€æœ‰ DataFrame
            dfs = [unify_columns(pd.read_parquet(f)) for f in chunk_files]
            chunk_df = pd.concat(dfs, ignore_index=True)
            
            # è½¬æ¢ä¸º Arrow Table
            table = pa.Table.from_pandas(chunk_df, preserve_index=False)
            
            if writer is None:
                # ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œåˆ›å»ºæ–‡ä»¶å’Œ ParquetWriter å¯¹è±¡
                writer = pq.ParquetWriter(FINAL_PARQUET_FILE, table.schema, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
            
            # å°†å½“å‰å—çš„ table å†™å…¥æ–‡ä»¶
            writer.write_table(table)
            
            print(f"\nå— {i//chunk_size + 1} å†™å…¥å®Œæˆã€‚")
            print_system_stats() # æ¯æ¬¡å†™å…¥åéƒ½æ‰“å°èµ„æºæƒ…å†µ

    finally:
        # ç¡®ä¿ writer è¢«å…³é—­
        if writer:
            writer.close()
            print("\nParquet writer å·²å…³é—­ã€‚")

    print(f"\nåˆå¹¶å†™å…¥å®Œæˆï¼Œæ­£åœ¨è¯»å–æœ€ç»ˆæ–‡ä»¶è¿›è¡Œæ’åºå’Œè´¨æ£€...")
    
    # --- é˜¶æ®µ 3: æœ€ç»ˆæ’åºä¸è´¨æ£€ ---
    # è¿™ä¸€æ­¥ä¾ç„¶æ˜¯å†…å­˜ç“¶é¢ˆï¼Œå¦‚æœæ•°æ®é‡è¿‡å¤§ï¼ˆå‡ åGBï¼‰ï¼Œè¿™é‡Œå¯èƒ½ä¾ç„¶ä¼šå¤±è´¥
    # ä½†å¯¹äºæ‚¨å‡ GBçš„æ•°æ®é‡ï¼Œ7GBå†…å­˜é€šå¸¸è¶³å¤Ÿ
    try:
        final_df = pd.read_parquet(FINAL_PARQUET_FILE)
        
        print("æŒ‰ code + date æ’åºä¼˜åŒ–å‹ç¼©...")
        final_df = final_df.sort_values(['code', 'date']).reset_index(drop=True)
        
        print(f"æ­£åœ¨é‡æ–°å†™å…¥å·²æ’åºçš„æœ€ç»ˆæ–‡ä»¶ï¼š{FINAL_PARQUET_FILE}...")
        final_df.to_parquet(FINAL_PARQUET_FILE, index=False, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
        print("æœ€ç»ˆæ–‡ä»¶å†™å…¥æˆåŠŸï¼")
        
        # ... (åœ¨è¿™é‡Œæ’å…¥æ‚¨å®Œæ•´çš„ run_quality_check å‡½æ•°å®šä¹‰) ...
        # run_quality_check(final_df)
        print("\n(è·³è¿‡è´¨æ£€æŠ¥å‘Šç”Ÿæˆï¼Œæ‚¨å¯ä»¥åç»­æ·»åŠ )")
        
    except Exception as e:
        print(f"\nâŒ åœ¨æœ€ç»ˆæ’åºæˆ–è´¨æ£€é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        traceback.print_exc()
        exit(1)
