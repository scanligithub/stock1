# scripts/collect_fundflow.py
# 2025-11-17 ç»ˆæå†…å­˜å®‰å…¨ç‰ˆï¼šé€šè¿‡æµå¼å†™å…¥å’ŒDuckDBå¤–éƒ¨æ’åºï¼Œå½»åº•è§£å†³å†…å­˜æº¢å‡ºé—®é¢˜

import os
import pandas as pd
import glob
from tqdm import tqdm
import json
from datetime import datetime
import shutil
import sys
import traceback

# å°è¯•å¯¼å…¥æ ¸å¿ƒåº“
try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    import duckdb
    PYARROW_DUCKDB_AVAILABLE = True
except ImportError:
    PYARROW_DUCKDB_AVAILABLE = False

# ==================== é…ç½® ====================
INPUT_BASE_DIR = "all_fundflow"
SMALL_OUTPUT_DIR = "fundflow_small"
# (ä¿®æ”¹) å¢åŠ ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å
TEMP_UNSORTED_FILE = "full_fundflow_unsorted.parquet"
FINAL_PARQUET_FILE = "full_fundflow.parquet"
QUALITY_REPORT_FILE = "data_quality_report_fundflow.json"

os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)

# ==================== ç³»ç»Ÿèµ„æºç›‘æ§å‡½æ•° (ä¿æŒä¸å˜) ====================
def print_system_stats():
    """æ‰“å°å½“å‰çš„å†…å­˜å’Œç¡¬ç›˜ä½¿ç”¨æƒ…å†µ"""
    # ... (æ­¤å‡½æ•°å†…å®¹ä¸æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    print("-" * 20)
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"  -> ğŸ“Š RAM Usage: {mem.used / (1024**3):.2f} GB / {mem.total / (1024**3):.2f} GB ({mem.percent}%)")
    except ImportError:
        print("  -> ğŸ“Š RAM Usage: psutil not installed.")
    try:
        disk = shutil.disk_usage("/")
        print(f"  -> ğŸ“Š Disk Usage: {disk.used / (1024**3):.2f} GB / {disk.total / (1024**3):.2f} GB ({disk.used / disk.total * 100:.1f}%)")
    except Exception as e:
        print(f"  -> ğŸ“Š Disk Usage: Failed to get info: {e}")
    print("-" * 20)


# ==================== ç»Ÿä¸€å­—æ®µä¿®å¤å‡½æ•° (æ‚¨çš„ç‰ˆæœ¬ï¼Œä¿æŒä¸å˜) ====================
def unify_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠä¸ªè‚¡(æ–°æµª)å’ŒæŒ‡æ•°(ä¸œè´¢)çš„å­—æ®µå½»åº•ç»Ÿä¸€"""
    # ... (æ­¤å‡½æ•°å†…å®¹ä¸æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    if 'open' in df.columns and 'close' not in df.columns:
        df = df.rename(columns={'open': 'close'})
    elif 'open' in df.columns and 'close' in df.columns:
        df = df.drop(columns=['open'])
    unwanted = ['high', 'low', 'volume', 'amount', 'pre_close', 'open_interest']
    df = df.drop(columns=[c for c in unwanted if c in df.columns], errors='ignore')
    required_cols = ['date', 'code', 'close', 'pct_change', 'turnover_rate', 'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
    for col in required_cols:
        if col not in df.columns: df[col] = pd.NA
    numeric_cols = ['close', 'pct_change', 'turnover_rate', 'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
    for col in numeric_cols:
        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')
    if 'date' in df.columns: df['date'] = pd.to_datetime(df['date'], errors='coerce')
    return df[required_cols]

# ==================== ä¸»æµç¨‹ ====================
def main():
    if not PYARROW_DUCKDB_AVAILABLE:
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ‰¾åˆ° 'pyarrow' æˆ– 'duckdb' åº“ã€‚è¯·åœ¨å·¥ä½œæµä¸­å®‰è£…å®ƒä»¬ã€‚")
        sys.exit(1)
        
    print("å¼€å§‹ èµ„é‡‘æµæ•°æ®æ”¶é›†ä¸åˆå¹¶æµç¨‹...")
    print_system_stats()

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    files = glob.glob(search_pattern, recursive=True)
    
    if not files:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç‰‡æ–‡ä»¶ï¼Œé€€å‡ºã€‚"); return
        
    print(f"å‘ç° {len(files)} ä¸ªèµ„é‡‘æµåˆ†ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # --- é˜¶æ®µ 1: å¤åˆ¶å°æ–‡ä»¶ (ä¿æŒä¸å˜) ---
    if os.path.exists(SMALL_OUTPUT_DIR): shutil.rmtree(SMALL_OUTPUT_DIR)
    os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)
    for f in tqdm(files, desc="å¤åˆ¶èµ„é‡‘æµå°æ–‡ä»¶"):
        filename = os.path.basename(f)
        shutil.copy2(f, os.path.join(SMALL_OUTPUT_DIR, filename))
    print(f"æ‰€æœ‰å°æ–‡ä»¶å·²æ”¶é›†è‡³ {SMALL_OUTPUT_DIR}/")

    # --- é˜¶æ®µ 2: æµå¼å†™å…¥æœªæ’åºçš„åˆå¹¶æ–‡ä»¶ (ä¿æŒä¸å˜) ---
    chunk_size = 2000
    writer = None
    print(f"\nå°†ä»¥æµå¼å†™å…¥æ¨¡å¼åˆå¹¶ï¼Œæ¯å— {chunk_size} ä¸ªæ–‡ä»¶...")
    try:
        for i in tqdm(range(0, len(files), chunk_size), desc="åˆ†å—å†™å…¥ Parquet ä¸­"):
            chunk_files = files[i : i + chunk_size]
            dfs = [unify_columns(pd.read_parquet(f)) for f in chunk_files]
            chunk_df = pd.concat(dfs, ignore_index=True)
            table = pa.Table.from_pandas(chunk_df, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(TEMP_UNSORTED_FILE, table.schema, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
            writer.write_table(table)
            print(f"\nå— {i//chunk_size + 1} å†™å…¥å®Œæˆã€‚")
            print_system_stats()
    finally:
        if writer:
            writer.close()
            print("\nParquet writer å·²å…³é—­ã€‚")

    # --- é˜¶æ®µ 3: ä½¿ç”¨ DuckDB è¿›è¡Œå†…å­˜å®‰å…¨çš„å¤–éƒ¨æ’åº (æ ¸å¿ƒä¿®æ­£) ---
    print(f"\nåˆå¹¶å†™å…¥å®Œæˆï¼Œæ–‡ä»¶ä¸º {TEMP_UNSORTED_FILE}ã€‚å‡†å¤‡ä½¿ç”¨ DuckDB è¿›è¡Œå¤–éƒ¨æ’åº...")
    print_system_stats()
    
    try:
        con = duckdb.connect()
        # DuckDB å¯ä»¥è®¾ç½®å†…å­˜é™åˆ¶ï¼Œå®ƒä¼šè‡ªåŠ¨ä½¿ç”¨ä¸´æ—¶ç£ç›˜æ–‡ä»¶æ¥å¤„ç†è¶…å‡ºçš„éƒ¨åˆ†
        con.execute("SET memory_limit='5GB';") 
        
        query = f"""
        COPY (
            SELECT * 
            FROM read_parquet('{TEMP_UNSORTED_FILE}')
            ORDER BY code, date
        ) TO '{FINAL_PARQUET_FILE}' (FORMAT PARQUET, COMPRESSION 'ZSTD');
        """
        
        print("... æ­£åœ¨æ‰§è¡Œ DuckDB æ’åºå’Œå†™å…¥ ...")
        con.execute(query)
        con.close()
        
        print(f"âœ… DuckDB æ’åºå®Œæˆï¼å·²ç”Ÿæˆæœ€ç»ˆæ’åºæ–‡ä»¶: {FINAL_PARQUET_FILE}")
        # åˆ é™¤ä¸´æ—¶çš„æœªæ’åºæ–‡ä»¶
        os.remove(TEMP_UNSORTED_FILE)
        print_system_stats()
        
    except Exception as e:
        print(f"\nâŒ åœ¨ DuckDB æ’åºé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        # å¦‚æœæ’åºå¤±è´¥ï¼Œå°è¯•å°†æœªæ’åºçš„æ–‡ä»¶ä½œä¸ºæœ€ç»ˆæ–‡ä»¶ï¼Œä»¥ä¾¿åç»­æ­¥éª¤å¯ä»¥ç»§ç»­
        if os.path.exists(TEMP_UNSORTED_FILE):
            os.rename(TEMP_UNSORTED_FILE, FINAL_PARQUET_FILE)
        # sys.exit(1) # æš‚æ—¶ä¸è®©å·¥ä½œæµå¤±è´¥

    # --- é˜¶æ®µ 4: ç”Ÿæˆè´¨æ£€æŠ¥å‘Š (æ‚¨çš„ç‰ˆæœ¬ï¼Œä¿æŒä¸å˜) ---
    print("\næ­£åœ¨è¯»å–æœ€ç»ˆæ–‡ä»¶è¿›è¡Œè´¨æ£€...")
    if os.path.exists(FINAL_PARQUET_FILE):
        final_df = pd.read_parquet(FINAL_PARQUET_FILE)
        
        print("\næ­£åœ¨ç”Ÿæˆè´¨æ£€æŠ¥å‘Š...")
        report = {
            "generate_time": datetime.now().isoformat(),
            "total_rows": len(final_df),
            "total_stocks": final_df['code'].nunique(),
            "date_range": {
                "min": final_df['date'].min().date().isoformat() if pd.notna(final_df['date'].min()) else None,
                "max": final_df['date'].max().date().isoformat() if pd.notna(final_df['date'].max()) else None
            },
            "columns": list(final_df.columns),
            "dtypes": final_df.dtypes.apply(lambda x: str(x)).to_dict()
        }
        with open(QUALITY_REPORT_FILE, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"è´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆï¼š{QUALITY_REPORT_FILE}")

        print("\nèµ„é‡‘æµå…¨å¸‚åœºæ•°æ®åˆå¹¶å®Œæˆï¼")
        print(f"â†’ æ€»è¡Œæ•°ï¼š{report['total_rows']:,}")
        print(f"â†’ è‚¡ç¥¨æ•°ï¼š{report['total_stocks']:,}")
        print(f"â†’ æ—¥æœŸèŒƒå›´ï¼š{report['date_range']['min']} ~ {report['date_range']['max']}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æœ€ç»ˆçš„ Parquet æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆè´¨æ£€æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        traceback.print_exc()
        exit(1)
