# scripts/collect_fundflow.py
# 2025-11-19 ç»ˆæåˆå¹¶ç‰ˆï¼šæµå¼å†™å…¥ + DuckDBæ’åº + é«˜çº§æ•°æ®è´¨é‡æ£€æŸ¥

import os
import pandas as pd
import glob
from tqdm import tqdm
import json
from datetime import datetime
import shutil
import sys
import traceback

# å°è¯•å¯¼å…¥æ ¸å¿ƒåº“
try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    import duckdb
    PYARROW_DUCKDB_AVAILABLE = True
except ImportError:
    PYARROW_DUCKDB_AVAILABLE = False

# ==================== é…ç½® ====================
INPUT_BASE_DIR = "all_fundflow"
SMALL_OUTPUT_DIR = "fundflow_small"
TEMP_UNSORTED_FILE = "full_fundflow_unsorted.parquet"
FINAL_PARQUET_FILE = "full_fundflow.parquet"
QUALITY_REPORT_FILE = "data_quality_report_fundflow.json"

os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)

# ==================== ç³»ç»Ÿèµ„æºç›‘æ§å‡½æ•° (ä¿æŒä¸å˜) ====================
def print_system_stats():
    # ... (æ­¤å‡½æ•°å†…å®¹ä¸æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    pass

# ==================== ç»Ÿä¸€å­—æ®µä¿®å¤å‡½æ•° (ä¿æŒä¸å˜) ====================
def unify_columns(df: pd.DataFrame) -> pd.DataFrame:
    # ... (æ­¤å‡½æ•°å†…å®¹ä¸æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    pass

# ==================== (æ–°å¢) é«˜çº§æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•° ====================
def run_advanced_quality_check():
    """
    ç›´æ¥åˆ†æ 'fundflow_small/' ä¸‹çš„æ‰€æœ‰ç‹¬ç«‹å°æ–‡ä»¶ï¼Œç”Ÿæˆé«˜çº§è´¨æ£€æŠ¥å‘Šã€‚
    """
    print("\n" + "="*50)
    print("ğŸ” [QC] å¼€å§‹è¿›è¡Œé«˜çº§æ•°æ®è´¨é‡æ£€æŸ¥...")
    
    small_files_path = os.path.join(SMALL_OUTPUT_DIR, "*.parquet")
    small_files = glob.glob(small_files_path)

    if not small_files:
        print("âš ï¸ [QC] æœªåœ¨ fundflow_small/ ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆè´¨æ£€æŠ¥å‘Šã€‚")
        return

    print(f"  -> [QC] å°†å¯¹ {len(small_files)} ä¸ªç‹¬ç«‹çš„è‚¡ç¥¨æ–‡ä»¶è¿›è¡Œåˆ†æ...")
    
    stock_reports = []
    total_records = 0
    total_error_records = 0
    all_dates = []

    for f in tqdm(small_files, desc="[QC] æ­£åœ¨åˆ†ææ¯åªè‚¡ç¥¨"):
        try:
            df = pd.read_parquet(f)
            if df.empty:
                continue

            # å‡è®¾æ–‡ä»¶åå°±æ˜¯è‚¡ç¥¨ä»£ç  (sh.600000.parquet)
            code = os.path.splitext(os.path.basename(f))[0]
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df.dropna(subset=['date'], inplace=True)
            
            start_date = df['date'].min()
            end_date = df['date'].max()
            record_count = len(df)
            total_records += record_count
            all_dates.extend([start_date, end_date])
            
            expected_dates = pd.date_range(start=start_date, end=end_date, freq='B')
            missing_days = len(expected_dates.difference(df['date']))
            
            flow_cols = ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
            error_rows = df[df[flow_cols].isnull().all(axis=1) | (df[flow_cols] == 0).all(axis=1)].shape[0]
            total_error_records += error_rows
            
            stock_reports.append({
                "code": code,
                "record_count": record_count,
                "start_date": start_date.strftime('%Y-%m-%d'),
                "end_date": end_date.strftime('%Y-%m-%d'),
                "missing_business_days": missing_days,
                "error_records_count": error_rows
            })
        except Exception as e:
            print(f"\nâš ï¸ [QC] åˆ†ææ–‡ä»¶ {f} å¤±è´¥: {e}")

    print("\n... [QC] æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š ...")
    final_report = {
        "generate_time": datetime.now().isoformat(),
        "total_stocks_processed": len(stock_reports),
        "total_records_analyzed": total_records,
        "total_error_records_found": total_error_records,
        "global_date_range": {
            "min": min(all_dates).strftime('%Y-%m-%d') if all_dates else None,
            "max": max(all_dates).strftime('%Y-%m-%d') if all_dates else None
        },
        "per_stock_details": stock_reports
    }

    with open(QUALITY_REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    print(f"âœ… [QC] é«˜çº§è´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆï¼š{QUALITY_REPORT_FILE}")

    print("\n--- èµ„é‡‘æµæ•°æ®è´¨é‡ç®€æŠ¥ ---")
    print(f"â†’ æ ‡çš„æ€»æ•°ï¼ˆåˆ†ææˆåŠŸï¼‰: {final_report['total_stocks_processed']:,}")
    print(f"â†’ æ€»è®°å½•æ•°ï¼š{final_report['total_records_analyzed']:,}")
    print(f"â†’ å¼‚å¸¸è®°å½•æ•°ï¼ˆå…¨ä¸º0æˆ–ç©ºï¼‰: {final_report['total_error_records_found']:,}")
    date_range = final_report.get('global_date_range', {})
    print(f"â†’ å…¨å±€æ—¥æœŸèŒƒå›´ï¼š{date_range.get('min')} ~ {date_range.get('max')}")

# ==================== ä¸»æµç¨‹ ====================
def main():
    if not PYARROW_DUCKDB_AVAILABLE:
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ‰¾åˆ° 'pyarrow' æˆ– 'duckdb' åº“ã€‚")
        sys.exit(1)
        
    print("å¼€å§‹ èµ„é‡‘æµæ•°æ®æ”¶é›†ä¸åˆå¹¶æµç¨‹...")
    print_system_stats()

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    files = glob.glob(search_pattern, recursive=True)
    if not files: print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç‰‡æ–‡ä»¶ï¼Œé€€å‡ºã€‚"); return
    print(f"å‘ç° {len(files)} ä¸ªèµ„é‡‘æµåˆ†ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # --- é˜¶æ®µ 1: å¤åˆ¶å°æ–‡ä»¶ ---
    if os.path.exists(SMALL_OUTPUT_DIR): shutil.rmtree(SMALL_OUTPUT_DIR)
    os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)
    for f in tqdm(files, desc="å¤åˆ¶èµ„é‡‘æµå°æ–‡ä»¶"):
        filename = os.path.basename(f)
        shutil.copy2(f, os.path.join(SMALL_OUTPUT_DIR, filename))
    print(f"æ‰€æœ‰å°æ–‡ä»¶å·²æ”¶é›†è‡³ {SMALL_OUTPUT_DIR}/")

    # --- é˜¶æ®µ 2: æµå¼å†™å…¥æœªæ’åºçš„åˆå¹¶æ–‡ä»¶ ---
    chunk_size = 2000
    writer = None
    print(f"\nå°†ä»¥æµå¼å†™å…¥æ¨¡å¼åˆå¹¶ï¼Œæ¯å— {chunk_size} ä¸ªæ–‡ä»¶...")
    try:
        for i in tqdm(range(0, len(files), chunk_size), desc="åˆ†å—å†™å…¥ Parquet ä¸­"):
            chunk_files = files[i : i + chunk_size]
            
            # (é‡è¦) åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸å†éœ€è¦è°ƒç”¨ unify_columnsï¼Œå› ä¸ºæˆ‘ä»¬ä¿¡ä»»ä¸Šæ¸¸
            # ä½†ä¸ºäº†ä¿®å¤ code ä¸¢å¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œå¤„ç†
            dfs = []
            for f in chunk_files:
                df = pd.read_parquet(f)
                # ä»æ–‡ä»¶åæ³¨å…¥ code
                filename = os.path.basename(f)
                code = os.path.splitext(filename)[0]
                df['code'] = code
                dfs.append(df)

            chunk_df = pd.concat(dfs, ignore_index=True)
            table = pa.Table.from_pandas(chunk_df, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(TEMP_UNSORTED_FILE, table.schema, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
            writer.write_table(table)
            print(f"\nå— {i//chunk_size + 1} å†™å…¥å®Œæˆã€‚")
            print_system_stats()
    finally:
        if writer:
            writer.close()
            print("\nParquet writer å·²å…³é—­ã€‚")

    # --- é˜¶æ®µ 3: ä½¿ç”¨ DuckDB è¿›è¡Œå†…å­˜å®‰å…¨çš„å¤–éƒ¨æ’åº ---
    print(f"\nåˆå¹¶å†™å…¥å®Œæˆ... å‡†å¤‡ä½¿ç”¨ DuckDB è¿›è¡Œå¤–éƒ¨æ’åº...")
    # ... (DuckDB æ’åºé€»è¾‘ä¿æŒä¸å˜) ...

    # --- é˜¶æ®µ 4: ç”Ÿæˆé«˜çº§è´¨æ£€æŠ¥å‘Š ---
    # è°ƒç”¨æ–°çš„ã€åŠŸèƒ½æ›´å¼ºå¤§çš„è´¨æ£€å‡½æ•°
    run_advanced_quality_check()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        traceback.print_exc()
        exit(1)
