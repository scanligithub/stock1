# scripts/collect_fundflow.py
# 2025-11-17 ç»ˆæå†…å­˜å®‰å…¨ç‰ˆï¼šé€šè¿‡æµå¼å†™å…¥è§£å†³è¶…å¤§æ–‡ä»¶åˆå¹¶æ—¶çš„å†…å­˜æº¢å‡ºé—®é¢˜

import os
import pandas as pd
import glob
from tqdm import tqdm
import json
from datetime import datetime
import shutil
import sys
import traceback

# å°è¯•å¯¼å…¥ pyarrowï¼Œå¦‚æœå¤±è´¥ï¼Œåé¢ä¼šå¤„ç†
try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False

# ==================== é…ç½® ====================
INPUT_BASE_DIR = "all_fundflow"
SMALL_OUTPUT_DIR = "fundflow_small"
FINAL_PARQUET_FILE = "full_fundflow.parquet"
QUALITY_REPORT_FILE = "data_quality_report_fundflow.json"

os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)

# ==================== ç³»ç»Ÿèµ„æºç›‘æ§å‡½æ•° ====================
def print_system_stats():
    """æ‰“å°å½“å‰çš„å†…å­˜å’Œç¡¬ç›˜ä½¿ç”¨æƒ…å†µ"""
    print("-" * 20)
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"  -> ğŸ“Š RAM Usage: {mem.used / (1024**3):.2f} GB / {mem.total / (1024**3):.2f} GB ({mem.percent}%)")
    except ImportError:
        print("  -> ğŸ“Š RAM Usage: psutil not installed.")
    
    try:
        disk = shutil.disk_usage("/")
        print(f"  -> ğŸ“Š Disk Usage: {disk.used / (1024**3):.2f} GB / {disk.total / (1024**3):.2f} GB ({disk.used / disk.total * 100:.1f}%)")
    except Exception as e:
        print(f"  -> ğŸ“Š Disk Usage: Failed to get info: {e}")
    print("-" * 20)


# ==================== ç»Ÿä¸€å­—æ®µä¿®å¤å‡½æ•° (æ‚¨çš„ç‰ˆæœ¬) ====================
def unify_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠä¸ªè‚¡(æ–°æµª)å’ŒæŒ‡æ•°(ä¸œè´¢)çš„å­—æ®µå½»åº•ç»Ÿä¸€"""
    if 'open' in df.columns and 'close' not in df.columns:
        df = df.rename(columns={'open': 'close'})
    elif 'open' in df.columns and 'close' in df.columns:
        df = df.drop(columns=['open'])

    unwanted = ['high', 'low', 'volume', 'amount', 'pre_close', 'open_interest']
    df = df.drop(columns=[c for c in unwanted if c in df.columns], errors='ignore')

    required_cols = [
        'date', 'code', 'close', 'pct_change', 'turnover_rate',
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow',
        'large_net_flow', 'medium_small_net_flow'
    ]
    for col in required_cols:
        if col not in df.columns:
            df[col] = pd.NA

    numeric_cols = [
        'close', 'pct_change', 'turnover_rate', 'net_flow_amount',
        'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # ç¡®ä¿è¿”å›çš„åˆ—é¡ºåºä¸ required_cols ä¸€è‡´
    return df[required_cols]

# ==================== ä¸»æµç¨‹ ====================
def main():
    if not PYARROW_AVAILABLE:
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ‰¾åˆ° 'pyarrow' åº“ã€‚è¯·è¿è¡Œ 'pip install pyarrow zstandard psutil'ã€‚")
        sys.exit(1)
        
    print("å¼€å§‹ èµ„é‡‘æµæ•°æ®æ”¶é›†ä¸åˆå¹¶æµç¨‹...")
    print_system_stats()

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    files = glob.glob(search_pattern, recursive=True)
    
    if not files:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç‰‡æ–‡ä»¶ï¼Œé€€å‡ºã€‚"); return
        
    print(f"å‘ç° {len(files)} ä¸ªèµ„é‡‘æµåˆ†ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # --- é˜¶æ®µ 1: å¤åˆ¶å°æ–‡ä»¶ (ä¿æŒä¸å˜) ---
    if os.path.exists(SMALL_OUTPUT_DIR):
        shutil.rmtree(SMALL_OUTPUT_DIR)
    os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)
    for f in tqdm(files, desc="å¤åˆ¶èµ„é‡‘æµå°æ–‡ä»¶"):
        filename = os.path.basename(f)
        shutil.copy2(f, os.path.join(SMALL_OUTPUT_DIR, filename))
    print(f"æ‰€æœ‰å°æ–‡ä»¶å·²æ”¶é›†è‡³ {SMALL_OUTPUT_DIR}/")

    # --- é˜¶æ®µ 2: æµå¼å†™å…¥ï¼ŒèŠ‚çœå†…å­˜ ---
    chunk_size = 2000 # æ¯æ¬¡å¤„ç† 2000 ä¸ªæ–‡ä»¶
    writer = None
    
    print(f"\nå°†ä»¥æµå¼å†™å…¥æ¨¡å¼åˆå¹¶ï¼Œæ¯å— {chunk_size} ä¸ªæ–‡ä»¶...")

    try:
        for i in tqdm(range(0, len(files), chunk_size), desc="åˆ†å—å†™å…¥ Parquet ä¸­"):
            chunk_files = files[i : i + chunk_size]
            
            dfs = [unify_columns(pd.read_parquet(f)) for f in chunk_files]
            chunk_df = pd.concat(dfs, ignore_index=True)
            
            table = pa.Table.from_pandas(chunk_df, preserve_index=False)
            
            if writer is None:
                writer = pq.ParquetWriter(FINAL_PARQUET_FILE, table.schema, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
            
            writer.write_table(table)
            
            print(f"\nå— {i//chunk_size + 1} å†™å…¥å®Œæˆã€‚")
            print_system_stats()

    finally:
        if writer:
            writer.close()
            print("\nParquet writer å·²å…³é—­ã€‚")

    # --- é˜¶æ®µ 3: æœ€ç»ˆæ’åºä¸è´¨æ£€ ---
    print(f"\nåˆå¹¶å†™å…¥å®Œæˆï¼Œæ­£åœ¨è¯»å–æœ€ç»ˆæ–‡ä»¶ {FINAL_PARQUET_FILE} è¿›è¡Œæ’åºå’Œè´¨æ£€...")
    
    try:
        final_df = pd.read_parquet(FINAL_PARQUET_FILE)
        
        print("æŒ‰ code + date æ’åºä¼˜åŒ–å‹ç¼©...")
        final_df = final_df.sort_values(['code', 'date']).reset_index(drop=True)
        
        print(f"æ­£åœ¨é‡æ–°å†™å…¥å·²æ’åºçš„æœ€ç»ˆæ–‡ä»¶ï¼š{FINAL_PARQUET_FILE}...")
        final_df.to_parquet(FINAL_PARQUET_FILE, index=False, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
        print("æœ€ç»ˆæ–‡ä»¶å†™å…¥æˆåŠŸï¼")
        print_system_stats()
        
        # --- é˜¶æ®µ 4: ç”Ÿæˆè´¨æ£€æŠ¥å‘Š (æ‚¨çš„ç‰ˆæœ¬) ---
        print("\næ­£åœ¨ç”Ÿæˆè´¨æ£€æŠ¥å‘Š...")
        report = {
            "generate_time": datetime.now().isoformat(),
            "total_rows": len(final_df),
            "total_stocks": final_df['code'].nunique(),
            "date_range": {
                "min": final_df['date'].min().date().isoformat() if pd.notna(final_df['date'].min()) else None,
                "max": final_df['date'].max().date().isoformat() if pd.notna(final_df['date'].max()) else None
            },
            "columns": list(final_df.columns),
            "dtypes": final_df.dtypes.apply(lambda x: str(x)).to_dict()
        }
        with open(QUALITY_REPORT_FILE, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"è´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆï¼š{QUALITY_REPORT_FILE}")

        print("\nèµ„é‡‘æµå…¨å¸‚åœºæ•°æ®åˆå¹¶å®Œæˆï¼")
        print(f"â†’ æ€»è¡Œæ•°ï¼š{report['total_rows']:,}")
        print(f"â†’ è‚¡ç¥¨æ•°ï¼š{report['total_stocks']:,}")
        print(f"â†’ æ—¥æœŸèŒƒå›´ï¼š{report['date_range']['min']} ~ {report['date_range']['max']}")

    except Exception as e:
        print(f"\nâŒ åœ¨æœ€ç»ˆæ’åºæˆ–è´¨æ£€é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        traceback.print_exc()
        exit(1)
