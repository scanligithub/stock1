# scripts/collect_fundflow.py
# 2025-11-19 æœ€ç»ˆå…¨åŠŸèƒ½ç‰ˆï¼šæµå¼å†™å…¥ + DuckDBæ’åº + é«˜çº§è´¨æ£€ + PyArrowå…¼å®¹æ€§ä¿®å¤ + å¢å¼ºå®‰æ£€é—¨

import os
import pandas as pd
import glob
from tqdm import tqdm
import json
from datetime import datetime
import shutil
import sys
import traceback

# å°è¯•å¯¼å…¥æ ¸å¿ƒåº“
try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    import duckdb
    PYARROW_DUCKDB_AVAILABLE = True
except ImportError:
    PYARROW_DUCKDB_AVAILABLE = False

# ==================== é…ç½® ====================
INPUT_BASE_DIR = "all_fundflow"
SMALL_OUTPUT_DIR = "fundflow_small"
TEMP_UNSORTED_FILE = "full_fundflow_unsorted.parquet"
FINAL_PARQUET_FILE = "full_fundflow.parquet"
QUALITY_REPORT_FILE = "data_quality_report_fundflow.json"

# å¤§æ–‡ä»¶é¢„è­¦é˜ˆå€¼ (MB)ï¼Œè¶…è¿‡æ­¤å¤§å°ä»…æ‰“å°æ—¥å¿—ï¼Œä¸è·³è¿‡
LARGE_FILE_WARNING_THRESHOLD_MB = 50 

os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)

# ==================== ç³»ç»Ÿèµ„æºç›‘æ§å‡½æ•° ====================
def print_system_stats():
    """æ‰“å°å½“å‰çš„å†…å­˜å’Œç¡¬ç›˜ä½¿ç”¨æƒ…å†µ"""
    print("-" * 20)
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"  -> ğŸ“Š RAM Usage: {mem.used / (1024**3):.2f} GB / {mem.total / (1024**3):.2f} GB ({mem.percent}%)")
    except ImportError:
        print("  -> ğŸ“Š RAM Usage: psutil not installed.")
    try:
        disk = shutil.disk_usage("/")
        print(f"  -> ğŸ“Š Disk Usage: {disk.used / (1024**3):.2f} GB / {disk.total / (1024**3):.2f} GB ({disk.used / disk.total * 100:.1f}%)")
    except Exception as e:
        print(f"  -> ğŸ“Š Disk Usage: Failed to get info: {e}")
    print("-" * 20)

# ==================== ç»Ÿä¸€å­—æ®µä¿®å¤å‡½æ•° ====================
def unify_columns(df: pd.DataFrame, code: str) -> pd.DataFrame:
    """æŠŠä¸ªè‚¡(æ–°æµª)å’ŒæŒ‡æ•°(ä¸œè´¢)çš„å­—æ®µå½»åº•ç»Ÿä¸€"""
    df['code'] = code
    
    if 'opendate' in df.columns:
        df = df.rename(columns={'opendate': 'date'})

    # (é‡è¦) ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹ä»¥ä¾¿åç»­æ“ä½œ
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # åŠ¨æ€æ„å»ºéœ€è¦è½¬æ¢å’Œä¿ç•™çš„åˆ—
    required_cols = [
        'date', 'code', 'close', 'pct_change', 'turnover_rate',
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow',
        'large_net_flow', 'medium_small_net_flow'
    ]
    
    final_df = pd.DataFrame()
    for col in required_cols:
        if col in df.columns:
            final_df[col] = df[col]
        else:
            final_df[col] = pd.NA
    
    numeric_cols = [c for c in final_df.columns if c not in ['date', 'code']]
    final_df[numeric_cols] = final_df[numeric_cols].apply(pd.to_numeric, errors='coerce')
    
    return final_df

# ==================== é«˜çº§æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•° ====================
def run_advanced_quality_check():
    """
    ç›´æ¥åˆ†æ 'fundflow_small/' ä¸‹çš„æ‰€æœ‰ç‹¬ç«‹å°æ–‡ä»¶ï¼Œç”Ÿæˆé«˜çº§è´¨æ£€æŠ¥å‘Šã€‚
    """
    print("\n" + "="*50)
    print("ğŸ” [QC] å¼€å§‹è¿›è¡Œé«˜çº§æ•°æ®è´¨é‡æ£€æŸ¥...")
    
    small_files_path = os.path.join(SMALL_OUTPUT_DIR, "*.parquet")
    small_files = glob.glob(small_files_path)

    if not small_files:
        print("âš ï¸ [QC] æœªåœ¨ fundflow_small/ ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆè´¨æ£€æŠ¥å‘Šã€‚")
        return

    print(f"  -> [QC] å°†å¯¹ {len(small_files)} ä¸ªç‹¬ç«‹çš„è‚¡ç¥¨æ–‡ä»¶è¿›è¡Œåˆ†æ...")
    
    stock_reports = []
    total_records = 0
    total_error_records = 0
    all_dates = []

    for f in tqdm(small_files, desc="[QC] æ­£åœ¨åˆ†ææ¯åªè‚¡ç¥¨"):
        try:
            df = pd.read_parquet(f)
            if df.empty:
                continue

            code = df['code'].iloc[0]
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df.dropna(subset=['date'], inplace=True)
            
            if df.empty: continue
            
            start_date, end_date = df['date'].min(), df['date'].max()
            record_count = len(df)
            total_records += record_count
            all_dates.extend([start_date, end_date])
            
            expected_dates = pd.date_range(start=start_date, end=end_date, freq='B')
            missing_days = len(expected_dates.difference(df['date']))
            
            flow_cols = ['net_flow_amount', 'main_net_flow'] # åªæ£€æŸ¥æ ¸å¿ƒæŒ‡æ ‡
            error_rows = df[df[flow_cols].isnull().all(axis=1) | (df[flow_cols] == 0).all(axis=1)].shape[0]
            total_error_records += error_rows
            
            stock_reports.append({
                "code": code,
                "record_count": record_count,
                "start_date": start_date.strftime('%Y-%m-%d'),
                "end_date": end_date.strftime('%Y-%m-%d'),
                "missing_business_days": missing_days,
                "error_records_count": error_rows
            })
        except Exception as e:
            print(f"\nâš ï¸ [QC] åˆ†ææ–‡ä»¶ {f} å¤±è´¥: {e}")

    print("\n... [QC] æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š ...")
    final_report = {
        "generate_time": datetime.now().isoformat(),
        "total_stocks_processed": len(stock_reports),
        "total_records_analyzed": total_records,
        "total_error_records_found": total_error_records,
        "global_date_range": {
            "min": min(all_dates).strftime('%Y-%m-%d') if all_dates else None,
            "max": max(all_dates).strftime('%Y-%m-%d') if all_dates else None
        },
        "per_stock_details": stock_reports
    }

    with open(QUALITY_REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    print(f"âœ… [QC] é«˜çº§è´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆï¼š{QUALITY_REPORT_FILE}")

    print("\n--- èµ„é‡‘æµæ•°æ®è´¨é‡ç®€æŠ¥ ---")
    print(f"â†’ æ ‡çš„æ€»æ•°ï¼ˆåˆ†ææˆåŠŸï¼‰: {final_report['total_stocks_processed']:,}")
    print(f"â†’ æ€»è®°å½•æ•°ï¼š{final_report['total_records_analyzed']:,}")
    print(f"â†’ å¼‚å¸¸è®°å½•æ•°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ä¸º0æˆ–ç©ºï¼‰: {final_report['total_error_records_found']:,}")
    date_range = final_report.get('global_date_range', {})
    print(f"â†’ å…¨å±€æ—¥æœŸèŒƒå›´ï¼š{date_range.get('min')} ~ {date_range.get('max')}")

# ==================== ä¸»æµç¨‹ ====================
def main():
    if not PYARROW_DUCKDB_AVAILABLE:
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ‰¾åˆ° 'pyarrow' æˆ– 'duckdb' åº“ã€‚")
        sys.exit(1)
        
    print("å¼€å§‹ èµ„é‡‘æµæ•°æ®æ”¶é›†ä¸åˆå¹¶æµç¨‹...")
    print_system_stats()

    search_pattern = os.path.join(INPUT_BASE_DIR, "**", "*.parquet")
    files = glob.glob(search_pattern, recursive=True)
    if not files: print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç‰‡æ–‡ä»¶ï¼Œé€€å‡ºã€‚"); return
    print(f"å‘ç° {len(files)} ä¸ªèµ„é‡‘æµåˆ†ç‰‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    # --- é˜¶æ®µ 1: å¤åˆ¶å°æ–‡ä»¶ (å¸¦å®‰æ£€é—¨) ---
    if os.path.exists(SMALL_OUTPUT_DIR): shutil.rmtree(SMALL_OUTPUT_DIR)
    os.makedirs(SMALL_OUTPUT_DIR, exist_ok=True)
    
    files_copied = 0
    skipped_files = 0
    
    for f in tqdm(files, desc="å¤åˆ¶èµ„é‡‘æµå°æ–‡ä»¶"):
        # [å®‰æ£€ 1] ä¸¥æ ¼æ£€æŸ¥æ–‡ä»¶ååç¼€ï¼Œé˜²æ­¢ core dump æˆ– git æ–‡ä»¶æ··å…¥
        if not f.lower().endswith(".parquet"):
            print(f"âš ï¸ [å®‰æ£€æ‹¦æˆª] è·³è¿‡é Parquet æ–‡ä»¶: {f}")
            skipped_files += 1
            continue
            
        # [å®‰æ£€ 2] æ£€æŸ¥æ–‡ä»¶å¤§å° (ä»…æŠ¥è­¦ï¼Œä¸è·³è¿‡)
        f_size = os.path.getsize(f)
        f_size_mb = f_size / (1024 * 1024)
        
        if f_size_mb > LARGE_FILE_WARNING_THRESHOLD_MB:
            print(f"âš ï¸ [å¤§æ–‡ä»¶æç¤º] æ–‡ä»¶ {os.path.basename(f)} å¤§å°ä¸º {f_size_mb:.2f} MB (è¶…è¿‡ {LARGE_FILE_WARNING_THRESHOLD_MB}MB)ï¼Œç¡®è®¤å¤åˆ¶ã€‚")
        
        # æ‰§è¡Œå¤åˆ¶
        filename = os.path.basename(f)
        shutil.copy2(f, os.path.join(SMALL_OUTPUT_DIR, filename))
        files_copied += 1
        
    print(f"\nâœ… å°æ–‡ä»¶æ”¶é›†å®Œæ¯•ã€‚æˆåŠŸ: {files_copied}, æ‹¦æˆªéParquet: {skipped_files}")
    
    # [æ–°å¢] è°ƒè¯•æ‰“å°ï¼šåˆ—å‡ºè¾“å‡ºç›®å½•ä¸­æœ€å¤§çš„å‰5ä¸ªæ–‡ä»¶
    print(f"ğŸ” [è°ƒè¯•] æ£€æŸ¥ {SMALL_OUTPUT_DIR} ç›®å½•ä¸­æœ€å¤§çš„æ–‡ä»¶:")
    try:
        output_files = glob.glob(os.path.join(SMALL_OUTPUT_DIR, "*"))
        # æŒ‰å¤§å°æ’åºï¼Œå–å‰5
        output_files.sort(key=os.path.getsize, reverse=True)
        for i, f in enumerate(output_files[:5]):
            size_mb = os.path.getsize(f) / (1024 * 1024)
            print(f"   {i+1}. {os.path.basename(f)} - {size_mb:.2f} MB")
        if not output_files:
            print("   (ç›®å½•ä¸ºç©º)")
    except Exception as e:
        print(f"   è°ƒè¯•æ£€æŸ¥å¤±è´¥: {e}")

    # --- é˜¶æ®µ 2: æµå¼å†™å…¥æœªæ’åºçš„åˆå¹¶æ–‡ä»¶ ---
    chunk_size = 2000
    writer = None
    print(f"\nå°†ä»¥æµå¼å†™å…¥æ¨¡å¼åˆå¹¶ï¼Œæ¯å— {chunk_size} ä¸ªæ–‡ä»¶...")
    try:
        # é‡æ–°è·å–åˆšåˆšå¤åˆ¶è¿‡å»çš„æ–‡ä»¶åˆ—è¡¨ï¼Œç¡®ä¿æ¥æºçº¯å‡€
        target_files = glob.glob(os.path.join(SMALL_OUTPUT_DIR, "*.parquet"))
        
        for i in tqdm(range(0, len(target_files), chunk_size), desc="åˆ†å—å†™å…¥ Parquet ä¸­"):
            chunk_files = target_files[i : i + chunk_size]
            
            dfs = []
            for f in chunk_files:
                try:
                    df = pd.read_parquet(f)
                    filename = os.path.basename(f)
                    code = os.path.splitext(filename)[0]
                    clean_df = unify_columns(df, code)
                    dfs.append(clean_df)
                except Exception as e:
                    print(f"è¯»å–æˆ–æ¸…æ´—æ–‡ä»¶ {f} å¤±è´¥: {e}")

            if not dfs: continue
            
            chunk_df = pd.concat(dfs, ignore_index=True)
            
            if 'date' in chunk_df.columns and pd.api.types.is_datetime64_any_dtype(chunk_df['date']):
                chunk_df['date'] = chunk_df['date'].dt.strftime('%Y-%m-%d')
                chunk_df['date'].replace({pd.NaT: None}, inplace=True)

            table = pa.Table.from_pandas(chunk_df, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(TEMP_UNSORTED_FILE, table.schema, compression='zstd' if 'zstandard' in sys.modules else 'snappy')
            writer.write_table(table)
            print(f"\nå— {i//chunk_size + 1} å†™å…¥å®Œæˆã€‚")
            print_system_stats()
    finally:
        if writer:
            writer.close()
            print("\nParquet writer å·²å…³é—­ã€‚")

    # --- é˜¶æ®µ 3: ä½¿ç”¨ DuckDB è¿›è¡Œå†…å­˜å®‰å…¨çš„å¤–éƒ¨æ’åº ---
    print(f"\nåˆå¹¶å†™å…¥å®Œæˆ... å‡†å¤‡ä½¿ç”¨ DuckDB è¿›è¡Œå¤–éƒ¨æ’åº...")
    try:
        con = duckdb.connect()
        con.execute("SET memory_limit='5GB';") 
        query = f"""COPY (SELECT * FROM read_parquet('{TEMP_UNSORTED_FILE}') ORDER BY code, date) TO '{FINAL_PARQUET_FILE}' (FORMAT PARQUET, COMPRESSION 'ZSTD');"""
        con.execute(query)
        con.close()
        print(f"âœ… DuckDB æ’åºå®Œæˆï¼å·²ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶: {FINAL_PARQUET_FILE}")
        os.remove(TEMP_UNSORTED_FILE)
    except Exception as e:
        print(f"\nâŒ åœ¨ DuckDB æ’åºé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}"); traceback.print_exc()
        if os.path.exists(TEMP_UNSORTED_FILE):
            os.rename(TEMP_UNSORTED_FILE, FINAL_PARQUET_FILE)

    # --- é˜¶æ®µ 4: ç”Ÿæˆé«˜çº§è´¨æ£€æŠ¥å‘Š ---
    run_advanced_quality_check()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒâŒâŒ åœ¨ main å‡½æ•°é¡¶å±‚æ•è·åˆ°è‡´å‘½å¼‚å¸¸: {e} âŒâŒâŒ")
        traceback.print_exc()
        exit(1)
